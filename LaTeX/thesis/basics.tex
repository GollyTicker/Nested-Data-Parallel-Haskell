
% Zitat: Auf den Schultern von Riesen.

This chapter will give an introduction into the basics needed to
understand this thesis. We will be introduced into functional programming
in Haskell to ease the understanding of the presented code.
Then we will cover Nested Data Parallelism (NDP) where
key insights are made and concepts exploit them will be presented.
Afterwards a short introduction into parallel complexity measures is given.
We will learn about work and depth complexities, how they relate
to the runtime duraiton and how they can be calculated.
Finally, \algo itself is presented - along with
a description on how it works.

\section{Haskell}
  Haskell is a general purpose, strongly-typed, purely-functional programming
  language. It has full type-inference, referential transparency, higher-order-functions
  and many more features. Traditionally Haskell programming goes by the lines of
  "write what you mean" and "let the compiler do the work for you".
  The former means the use of high abstractions to write concise code -
  while the latter means the use of sophisticated compilers
  \footnote{some which - like the Glasgow Haskell Compiler -  are written in Haskell themselves!}
  to optimise the abstractions to efficient mashine code.
  
  We will now be briefly presented with the main features used in this thesis:
  \paragraph{Syntax}
  Consider this function:
  \begin{lstlisting}
foo :: Double -> Int
foo x = round (max 5 x)
  \end{lstlisting}
  \c{foo} takes the maximum of 5 and the argument number \c{x} and
  rounds it to the nearest integer. The function is given a type
  signature in the first line. \c{f :: t} denotes that \c{f} is of type \c{t}.
  The type describes a function from a double floating number
  to an integer. Multiple argument functions are typed with multiple arrows.
  E.g. \c{max :: Double -> Double -> Double}-
  In contrast to other programming languages parentheses \c{()} are not used for function application
  (as in \c{f(a)}) but instead are used to specify precedence/ordering.
  Function application is done with the most space efficient operator
  one can imagine - that is space ' ' itself! E.g. \c{round 5.2} applies the built-in
  float-rounding function to return the integer \c{5}. Multiple-argument
  functions can be applied with multiple spaces. E.g. \c{max 5 x}
  returns the maximum of 5 and x.
  
  
  We can write the function alternatively as:
  \begin{lstlisting}
foo :: Double -> Int
foo x = let a = max 5 x
                  b = round a
            in  b
  \end{lstlisting}
  \c{let} statements bind expressions to variables.
  The expression after the \c{in} keyword is the return value of the
  entire expression.
  
  
  The upper function can also be written without the explicit use of \c{x}.
  \begin{lstlisting}
foo = (round) . (max 5)
    = round . max 5   -- space has higher precedence than infix operators like .
  \end{lstlisting}
  This is called currying and function composition.
  We can ommit the last argument of a function and define the function
  \c{foo} by literally writing out what we mean:
  "Take the maximum of 5 and then round".
  Function composition is denoted by a dot \c{(.)}.
  
  In Haskell one often ends up nesting multiple functions:
  \begin{lstlisting}
myFunc x = (bar a b (baz c (foo d x)))
  \end{lstlisting}
  We can write the same function in various different ways - depending on space
  and visual ease.
  \begin{lstlisting}
myFunc x = bar a b . baz c . foo d $ x  -- composed with explicit argument
myFunc = bar a b . baz c . foo d        -- composed with implicit argument
myFunc x = bar a b                      -- indented, explicit argument
               . baz c 
               . foo d
               $ x
myFunc = bar a b                        -- indented, implicit argument
             . baz c
             . foo d
  \end{lstlisting}
    The new \c{\$} operator is used here. It - just like the space operator
    - also denotes function application
    \footnote{Fun fact: \c{\$} is defined in Haskell itself! It goes by \c{f \$ x = f x}}
    . However, it
    has very low precedence - so that the functions are composed first
    before the argument is applied into the pipeline of functions.
    Be reminded, that the multi-line definitions actually apply
    the functions from bottom to up. That means, first foo, then baz
    and finally bar is applied - even though foo is written last.
    
  \paragraph{Naming Conventions}
    Higher order functions are denoted with \c{f},\c{g} or \c{h}.
    Usual variables are denoted with \c{x},\c{y},\c{z},\c{a},\c{b},\c{c}
    or with longer names like \c{divisor} or \c{multWith2}.
    If two variables are closly related, then we
    can also write them as \c{x} and \c{x'}. The prime is also
    a part of the variable name.
    Finally, a collection of \c{x} will be refered to by \c{xs}.
    E.g. \c{xs} is an array, \c{xss} is a two dimensional array, and so on.
  
  \paragraph{Strong typing and Type Inference}
    Every expression in has a type. But the programmer doesn't need to
    specify them as they can be (almost always) inferred from the context.
  \paragraph{Type Synonyms}
    Type synonyms enable us to refer to a complicated type by a simpler name.
    E.g. \c{type Foo a = (Int,a)} defines a type synonym - such that
    \c{Foo Double} is replaced by \c{(Int,Double)} during compilation. Compare them to
    type macros.
  
  \paragraph{Referential Transparency and Immutability}
    Functions and expressions are referentially transparent in Haskell 
    (as opposed to 'procedure' or 'object').
    Functions and expressions - like their inspiring equivalent in
    Mathematics - always return the same result when evaluated.
    A consequece is Immutability - variables and data structures
    cannot be mutated. Each function always returns a new copy
    of the data structure. This enables inlining and rewrite rules
    as explored in section \ref{ndpintro}.
    
  \paragraph{Record Syntax}
    We can define records to combine multiple values into a single complex value.
    The following defines a record of type \c{Person} with the
    fields \c{name} and \c{age}
    \begin{lstlisting}
Person {
  name = "Mark"
  age = 23
}
    \end{lstlisting}
    
  \paragraph{Polymorphism}
    Types can be polymorphic. E.g. we can write the general function
    \begin{lstlisting}
foo :: a -> b -> a
foo x y = x
    \end{lstlisting}
    which takes two arguments and simply returns the former only.
    Lower case letters in the type signature denote type variables
    This function works for any arguments - regardless of their types -
    as long as the return type is same as the type of \c{x}. This
    restriction is reflected in the type signature - the first and
    the last type variable have to be equal.
    
    Similarly, we can define functions over polymorphic data types.
    E.g. \c{List a} denotes a list of elements of type a. \c{List Int}
    is for example a linked list of integers.
  
  \paragraph{Higher Order Functions}
    We can define functions which take and pass entire functions as arguments.
    E.g.\c{applyTwice} applies the function twice.
    \begin{lstlisting}
applyTwice :: (a -> a) -> a -> a
applyTwice f x = f (f x)
    \end{lstlisting}
    Note the extra parentheses in the type signature. The type signature
    states "Give me a function from a type \c{a} to itself and an
    argument of type \c{a} then I can return you a value of the same type."
    
  \paragraph{Lambdas}
    We can write literal functions by using lambda expressions.
    For example, the function \c{f x = 2*(x+1)} can also be written
    directly as \c{f = \lam x -> 2*(x+1)} or \c{f = \textbackslash{} x -> 2*(x+1)}.
    The backslash \textbackslash{} is used, when a lambda cannot be written out.
    \footnote{If you bend \textbackslash{} strong enough, you can see its similarity to \lam.}
    
  \paragraph{Identity, flip and function composition (.)}
    Three commonly used functions are:
    
    \begin{lstlisting}
id :: a -> a
id x = x
flip :: (a -> b -> c) -> b -> a -> c
flip f x y = f y x
(.) :: (b -> c) -> (a-> b) -> a -> c
g . f = \x -> g (f x)
    \end{lstlisting}
    where \c{id} is the identity function and \c{flip} exchanges a functions
    first two arguments.
    The identity function is the neutral element of function composition \c{(.)}.
    That menas that (among others) the following law holds:
     \c{\textrm{$\forall$} f: f . id = id . f = f}. This is going to be useful later.

  After a brief overview of the features and a helping hand on the syntax
  I will be using, we can take a look at Nested Data Parallelism.

\section{Nested Data Parallelism}
  \label{ndpintro}
  In the ground breaking work \cite{Belloch1996}
  major contributions to parallel programming in
  functional programming languages were made.
  The paper presented some of Belloch's earlier work (\cite{NepaBelloch1993}) on NESL
  - a programming language specifically designed for expressing parallelism
  in functional programming languages. Its ideas and insights were
  adapted to various languguages - one of them being Haskell.
  Multitude years of research was nesessary to generalize the
  advantages of the special purpose language NESL to a widely used
  general purpose language like Haskell.
  \footnote{For example \cite{Harness2008}, \cite{DPHStatus2007},
  \cite{EffiVect2012Lipp}, \cite{HighOrdFlat2006} and \cite{DistTypes1999}.}
  This project is called Nested Data Parallel Haskell and this section will give
  an overview of its key concepts.  
  
  \paragraph{}
    In Flat Data Parallelism, we are provided with parallel mapping primitives
    to express parallelism. For example, we might have a built-in
    function \c{map} such that \c{map f xs} applies \c{f}
    on each of the elements in the array \c{xs} in parallel.
    
    This function has a farily intuitive parallel implementation -
    simply distribute the input array evenly across the \underline{p}rocessing \underline{u}nits (PUs),
    compute each local chunk of the array with its PU and finally
    join all elements together.
    The inner function \c{f} is applied by each processing unit (PU) individually.
    It is therefore executed sequentially.
    
    This is no problem, if we have statements like \c{map incr}
    \footnote{where \c{incr} increments an integer.}
    . However, what shall we do if we have an expression like \c{map (map incr)}?
    Or even \c{map (map (map incr))}?
    We can only execute the outermost level in parallel because we have already
    distributed the workload among the PUs.
    That means, that the inner function \c{f} can only be executed squentially.
    
    
    This is where NDP can shine. In NDP, in contrast, \c{f} itself can
    also be a parallel operation - and all levels of nesting can
    be executed in parallel! Hence the name - \textbf{Nested} Data Parallelism.
    It does that by transforming the program we wrote into a functionally
    equivalent flat data parallel program. This (non-trivial) transformation
    is called 'flattening' or 'vectoriation' and includes subsequent compiler optimisations.
    All in all, the entire program transformation can be broken down in three steps.
    Each step introduces its new data types.
    \begin{enumerate}
      \item \emph{Vectorization} -
        Uses flat type-dependent array \pav{} representations instead of the
        vanilla nested parallel arrays \pan{} which are used by the programer.
        The flattening is also applied to nested parallel functions as
        those mentioned previously.
      \item \emph{Communicaiton Fusioning} -
        By inlining the definitions of the parallel functions and
        using semantics-preserving rewrite rules one can
        reduce unsessesary synchronisation points and
        create tight pipelines. It uses \pad{} to denote
        distributed chunks of a global array \pav{}.
      \item \emph{Stream Fusioning} -
        Sequential functions local to each PU can further
        be optimised similarly to the previous step
        to reduce the number of intermediate arrays and
        to create tight loops.
        It uses \type{Vector a} and \c{Stream a} to
        implement the local array chunks and to fuse loops, respectively.
    \end{enumerate}
    
  After a overview of the predefined functions - these steps
  are going to be explained in more detail.
  
  \paragraph{}
    For parallel arrays, we have a few predefined operations available.
    They correspond to functions used in conventional functional programming.
    Their names and their types are given in \ref{table:parfuns}.
    
    \begin{table}[h]
      \caption{Parallel functions in NDP}
      \label{table:parfuns}
      \begin{tabular}{lll}
          \toprule
          function & type & description \\
          \midrule
          (!:),indexP & \c{[:a:] -> Int -> a} & indexes the array \\
          lengthP & \c{[:a:] -> Int} & returns the length of the array \\
          headP & \c{[:a:] -> a} & returns the first element\\
          lastP & \c{[:a:] -> a} & returns the last element \\
          mapP & \c{(a -> b) -> [:a:] -> [:b:]} & applies a function on all elements \\
          zipWithP & \c{(a -> b -> c)} & zip together pairs of elements with a \\
           & \c{-> [:a:] -> [:b:] -> [:c:]} & function and return an array of results
           \footnote{e.g. \c{zipWithP (*) [:1,2,3:] [:1,3,4:] = [:1*1,2*3,3*4:]}. The arrays as assumed to be of equal size.}
           \\
          sortP & \c{[:Int:] -> [:Int:]} & sorts an array \\
          sumP & \c{[:Int:] -> Int} & summes the arrays elements \\
          concatP & \c{[:[:a:]:] -> [:a:]} & removes a level of nesting \\
          unconcatP & \c{[:[:a::]] -> [:b:] -> [:[:b:]:]} & exposes a structure to a flat array \\
          replP & \c{Int -> a -> [:a:]} & replicates an element \\
      \end{tabular}
    \end{table}
    
  \subsection{Vectorization}
    Vectorization roughly involves two major steps -
    \emph{Type dependent representation} and \emph{Lifting}.
    Both will be explained in detail now.
  
    \subsubsection{Type dependent representation}
      Type dependent representation 
      allows the programer to work with arrays containing complex
      data structures without sacrifying efficiency.
      
      Consider for example \type{[:Int:]} and \type{[:[:Int:]:]}.
      The former can be easily implemented using a contiguos region
      of memory and inserting the bytes. The latter however cannot
      be implemented equally. What if the sub-arrays are of unequal length?
      To allocate a block of memory, we need to know the size
      of all arrays beforehand.
      Implementing nested array naively would use an array of pointers to
      to arrays of integers. Pointers however are bad, since they
      decrease cache locality and therefore decrease overall performance.
      How can we implement a nested array without using many pointers?
      
      The solution is separation of data and structure.
      The folowing example describes how a nested array
      like \c{[[1,2,3],[4,5],[],[6]] :: [:[:Int:]:]} would be implemented:
      \footnote{\c{[\string#1,2,3\string#]} is the notation used for a contiguos-memory array.}
      \begin{lstlisting}
array :: PA (PA Int)
array = AArr {
  data = [# 1,2,3,4,5,6 #],
  indices = [# 0,3,5,5 #]
  lengths = [# 3,2,0,1 #]
}
      \end{lstlisting}
      All data is packed together into a data field - regardless of nesting -
      and all structucal information is divied into two byte-arrays.
      The indices array contains - for every subarray in the 
      original array - the index of the first element if it were
      to be indexed into the \c{data} field. The lengths describe the lengths of
      each subarray. Each subarray corresponds to a pair of index and length.
      For example, the \underline{second} subarray (\c{[4,5]}) corresponds to
      the \underline{second} index (3) and \underline{second} length (2). Extracting
      2 elements starting at index 3 in the \c{data} array will return
      exactly these two elements.
      Another thing to note; this flat representation
      also uses the new type \type{PA (PA Int)}
      instead of the old \type{[:[:Int:]:]}. 
      Astoundingly, for nested arrays the functions \c{concatP} and \c{unconcatP}
      become constant time operations!
      Removing a level of nesting simply means discarding the segment descriptors -
      and adding a nesting structure of an existing array to a flat array
      means simply adding the segment descriptor of the nested array to the
      flat array.
      
      These crucial insights - namely the flat representation
      of irregularly nested arrays and the constant time
      (un-)flattening operations will be very important later.
      
      
      There are implementation for all other types - such as arrays of ints,
      arrays of doubles, etc. For example \c{[:1,2,3:] :: [:Int:]} can be simply
      represented as:
      \begin{lstlisting}
array :: PA Int
array = AInt [# 1,2,3 #]
      \end{lstlisting}
      
      To finish the data flattening, compiler also transforms
      all normal parallel functions (like \c{fooP}) to
      their predefined scalar counterparts (like \c{fooPS}).
      The scalar functions work directly on the flat representation
      and are designed to exploit them for efficieny.
      This is exactly what \c{concatPS} and \c{unconcatPS} do.
      In fact, functions like \c{fooP} have no actual implementation
      because they are entirely replaced by their \c{fooPS} counterparts.
      They are only used to simpilfy the programmer view.
    
    \nomenclature{Cache Locality}{TODO}
      
    \subsubsection{Lifting}
      During lifting where all occurrences of \c{mapPS f} are
      replaced by \c{fL} and where \c{fL}
      is the lifted version of the original function.
      Compared to the original scalar function \c{f :: a -> b}, the lifted
      function applies the mapping over arrays - alas \c{fL :: PA a -> PA b}.
      For user-defined functions, this lifted function is defined
      by lifting the definition of \c{f} recursively.
      The key is now the definition of the lifted functions for the built-in functions.
      The most important one among them is lifted \c{mapPS} - namely \c{mapPL}.
      To \c{mapPS} of type \c{(a -> b) -> PA a -> PA b},
      \c{mapPL} is of type \c{(a -> b) -> PA (PA a) -> PA (PA b)}.
      To overcome the limitations of flat data parallelsim,
      its implementation has to map over all elements at once
      (instead of mapping over the outer-nesting level only).
      The following implementation does that.
    \begin{lstlisting}
mapPL :: (a -> b) -> PA (PA a) -> PA (PA b)
mapPL f xss =
  unconcatPS xss
  . fL
  . concatPS
  $ xss
    \end{lstlisting}
    The definition of \c{mapPL} is curcial. It is implemented by flattening the array (line 5), applying a flat data-parallel operation (line 4)
    and finally unflattening the new array into the original structure (line 3).
    \footnote{This implementation is different from the original in one important aspect - 
    namely the handling of presupplied arguments in the function. Our
    implementation only handles functions without presupplied arguments -
    but it much simpler to understand than the original in \cite{Harness2008}.
    In the real version we would have another expression \c{expandPS xss fEnv}.
    Introducing it wouldn't affect the overall situation in this thesis -
    so it was simplified.
    }
    Figure \ref{figure:mapP} shall visually aid the understanding by using
    an example of \c{mapP (mapP incr)}.
    
    \begin{figure}[h!]
        \begin{center}
        \includegraphics[width=\linewidth]{mapP.png}
        \caption{Conceptional programer view and actual implementation of \c{mapP (mapP incr)}
        \footnote{Here, the commas inbetween the elements have been omitted for visual clarity.
        This thesis will sometimes use such spaces for separation
        instead of commas.
        }
        }
        \label{figure:mapP}
        \end{center}
    \end{figure}
    
    This is the key insight in NDP! Nested Parallel operations - like \c{mapP (mapP incr)} are
    transformed into calls of \c{mapPL incrL} which avoid the nesting of
    the arrays entirely and enable us to map over the entire array in parallel.
    Using this precedure, we can transform nested data parallel programs
    to flat data parallel programs. The former is easy to write and flexible
    - while the latter is simple to implement and efficient in speed.
            
    Vectorization itself is a complex transformation - and many details were ommited.
    More details can be found in \cite{Harness2008}.
    
  \subsection{Communication Fusion}
    During Communication Fusion lifted functions like \c{incrL} are inlined.
    They generally have following form:
    \begin{lstlisting}
incrL :: PA Int -> PA Int
incrL = joinD . mapD incrS . splitD
    \end{lstlisting}
    The types of the new functions are explained in \ref{mapPs}.
    Essentially, the lifted functions are implemented by explicitly
    splitting the array across all PUs, apllying the original
    function (in this case \c{incrS}) on each local chunk sequentially and finally joining all chunks.
    The type \c{Dist a} denotes a distributed value \c{a}.
    Distributable types can be for example arrays or linked lists.
    In case of arrays, the array is chunked and distributed evenly.
    In case of linked lists, the lists can be chunked and
    inter-PU pointers can be used when each chunk reaches its end.
    
    Another aspect to note is the following distinction:
    Using \c{PA a} generally means, that the entire array
    is currently being processed by a single PU and
    then redistributed globally to all PUs.
    Using \c{Dist a} generally means,
    that each PU is currently processing its
    chunk of the distributed value locally.
    
    With such introductions, we can take a look at the type of expressions
    Communication Fusion is designed to optimise. Here is an example.
    \begin{lstlisting}
myFunc :: PA Int -> PA Int
myFunc = mult2L . incrL
    \end{lstlisting}
    where \c{mult2L} is a function which doubles each element in the array.
    Currently, the function works in two steps. First, it splits
    the array, increments and joins the array. Second, it split
    the array, doubles and joins the array again. This is however
    not the optimal approch - and it will be optimised by the compiler.
    
    An effective optimisation strategy in referentially transparent
    programming languages like Haskell are \emph{Inlining}
    \footnote{\cite{Inlining2002}}
    and \emph{Rewrites Rules}
    \footnote{\cite{Simon2001Rewrites}}
    .
    Inlining does what it literally says - it inlines definitions
    of functions and variables. Inlining the definitions
    of the lifted functions reveals the following:
    \begin{lstlisting}
myFunc :: PA Int -> PA Int
myFunc = joinD . mapD mult2S . splitD . joinD . mapD incrS . splitD
    \end{lstlisting}
    Then rewrite rules are used.
    They allow the specification of general semantic-preserving
    laws and allow the compiler to rewrite parts of the code
    according to them.
    They are written by humans and table \ref{rules} introduces a few of them.
    Among them is "splitD/joinD". It states the general law,
    that joining and resplitting an array doesn't change the array at all.
    This is clear for us - but not for the compiler. By specifying such rules
    we can aid the compiler in using equational reasoning to
    optimise the code. In our case the compiler finds the splitD/joinD pair and applies the rule to get:
    \begin{lstlisting}
myFunc :: PA Int -> PA Int
myFunc = joinD . mapD mult2S . mapD incrS . splitD
    \end{lstlisting}
    Applying the rules, the compiler just eliminated
    communication inbetween two phases of computation. This is marvelous!
    Using another rule, namely "mapD/mapD", we can further optimise to.
    \begin{lstlisting}
myFunc :: PA Int -> PA Int
myFunc = joinD . mapD (mult2S . incrS) . splitD
    \end{lstlisting}
    This is even better!
    Communication Fusion did not only reduce the communication but also
    packed together consecutive operations. Now we can further
    optimise local operations using Stream Fusion.
    
    
  \subsection{Stream Fusion}
    Stream Fusion, the optimisation of recursive composed functions
    into a single loop, is a complex topic
    \footnote{A few papers on Stream Fusion:
    \cite{GenVectorFusion2013}, \cite{Fusion2007} and \cite{ArrayFusion2001Chakravarty}
    }
    . We will not go into its details.
    For our purposes it is sufficient to know the following:
    \begin{itemize}
      \item Stream Fusion works very similar to communication fusion
      \item instead of inlining \c{fooL} functions, we inline \c{fooS} functions
      \item instead of mergeing "splitD/joinD" we merge "unstream/stream"
      \item instead of joining "mapD/mapD" we join "mapS/mapS"
      \item \c{Stream a} is a special stream-ful collection with
        non-recursive implementations of various library functions (e.g. \c{mapP},\c{groupP})
        over streams. It is a crucial for the internal optimisation.
      \item Any streams left over after optimisations are converted back into.
        \c{Vector a}. (Since they are the more efficient data container).
    \end{itemize}
    Applying the same procedure creates the following code:
    \begin{lstlisting}
myFunc :: PA Int -> PA Int
myFunc = joinD . mapD (mapS (mult2 . incr)) . splitD
    \end{lstlisting}
    In contrast to our prior - two step - distributed computation,
    our new function has been optimised to communicate only
    at the beginning and the end and to apply both mappings
    in a single loop per PU!
    This is exactly what "let the compiler do the work for you" means!
    
  \subsection{Tables of Functions and Rewrite Rules}
    The transformation in chapter \ref{chapter:ndpv} is going to
    make use of many functions. Most of them are going to be
    introduced when they first appear - others are more frequent
    and require a summary. They are explained in table \ref{mapPs}.
    The table also holds for other functions with same suffix - e.g.
    the \c{mapP} row similarly applies to \c{scanlP}, \c{groupP} etc.
    
    \begin{table}[h!]
      \caption{Overview of Functions and Phases in NDP}
      \label{mapPs}
      \begin{tabular}{lll}
          \toprule
          function & first appearance/ & type/ \\
            & execution context & description \\
          \midrule
          mapP f xs & Programer-view & \type{(a -> b) -> [:a:] -> [:b:]} \\
           & not executed & \textbf{P}arallel array functions the programer uses \\
          mapPS f xs & Vectorization & \type{(a -> b) -> PA a -> PA b} \\
           & not executed & \textbf{P}arallel \textbf{S}calar functions the program is vectorized into \\
          indexPL is xs & Vectorization & \type{PA Int -> PA (PA a) -> PA a} \\
           & not executed & \textbf{P}arallel \textbf{L}ifted indexing on flat arrays. It's scalar\\
           & & function is \type{indexP :: Int -> [:a:] -> a} \\
          mapD f x & Communication Fusion & \type{(Vector a -> Vector b)} \\
           & mapD: all PUs & \type{ -> Dist (PA a) -> Dist (PA b)} \\
           & & Each PU applies \c{f} on it's chunk of the \\
           & f: local per PU & \textbf{d}istributed value. True parallelsim here! \\
          fooS f xs & Communication Fusion & \type{Vector a -> Vector b}\\
           & local PU & Applies a function \textbf{s}equentially. \\
           & & \c{Vector} is the Haskell implementation of \\
           & & conventional PU-local in-memory arrays. \\
          splitD & Communication Fusion & \type{PA -> Dist (PA a)}\\
           & all PUs & splits a glboal array into chunks and \\
           & & distributes them to each of the PUs \\
          joinD & Communication Fusion & \type{Dist (PA a) -> PA a} \\
           & allPUs & join a distributed array into a global array \\
          stream & Stream Fusion & \type{Vector a -> Stream a}\\
           & not executed & convert from an array to a stream-ful data container \\
          unstream & Stream Fusion & \type{Stream a -> Vector a}\\
           & not executed & convert back to an array \\
          mapSt f xs & Stream Fusion & \type{(a -> b) -> Stream a -> Stream b}\\
           & not executed & Applies a function on a \textbf{st}ream of values \\
      \end{tabular}
    \end{table}

    \nomenclature{\c{Vector}}{is the Haskell implementation of conventional local in-memory arrays.
            They are used as the distributed chunks of \pad.}
    
    Most of the functions are simply inlined during optimisation.
    They don't exist anymore at runtime and
    therefore are marked 'not-executed'. Besides these functions, we will also need handful rewrite rules.
    The most important of them are described in table \ref{rules}.
    
    \begin{table}[h!]
      \caption{Rewrite Rules in NDP}
      \label{rules}
      \begin{tabular}{lll}
          \toprule
          rule & rewrite & description \\
          \midrule
          splitD/joinD & splitD . joinD = id & Joining and spliting an distributed array  \\
          & & doesn't change the contents. Therefore \\
          & & it is equivalent to doing nothing. \\
          
          mapD/mapD & mapD g . mapD f & Two consecutive mappings are \\
          & = mapD (g . f) & equivalent to a single mapping \\
          & & with both of the functions \\
          
          mapD/replD & mapD f . replD n & Replicating a value and mapping \\
          & = replD n . f & all values is equivalent to \\
          & & directly applying the function and \\
          & & subsequently replicating it. \\
          
          splitD/replPS & splitD . replPS n & Replicating and spliting a value \\
          & = replD n & is equivalent to creating the local chunks\\
          & & directly. ReplD implements this and knows \\
          & & which chunk its PU is responsible for. \\
          
          ZipReplSplit & zipWithD f (replD a) & Zipping with a replicated value\\
          & . splitD  & already in scope - is equivalent to - applying \\
          & = mapD (f a) & the mapping with the value \\
          & & curried into the function.\\
          
          mapD/zipWithD & mapD f . zipWithD g xs = & A map operation after a zip operation \\
          & zipWithD (\lam x y -> f (g x y)) xs & is equivalent to a single zip operation \\
          & & which applies both \c{f} and \c{g}. \\
          
          unstream/stream & unstream . stream = id & Converting back and fourth is \\
          &  & equivalent to doing nothing. \\
       \end{tabular}
    \end{table}
  
  \subsection{A word on accuracy}
    The project of NDP in Haskell is - even after 15 years -
    still in \textit{work in progress}. Due to frequent changes,
    the papers often use conflicting notation and refer to
    different statuses of progress. Inconsistent literature and a project still in work makes it difficult
    to apply it in a thesis. It is not simple to use the original ideas from
    NESL directly on Haskell as there are great differences (not mentioning
    the fact it already took 15 years to adapt).
    
    Therefore, in this thesis, I have improvised on various conflicting or
    missing details. I assumed implementations which could really have been
    used in NDP.\footnote{E.g. \c{groupP} as introducted in chapter \ref{chapter:ndpn} doesn't
    even exist right now. However, its implementation described there is perfectly possible.}
    The reader is hereby noted that the details
    mentioned here are implementable - but not nesessarily an accurate
    representation of the current state of progress.
    
    
  \paragraph{}
    We have now finally been introduced to the details of NDP and
    are now ready to proceed. The new sections will explain
    parallel complexitiy measures and \algo itself.
  
  \clearpage
  
\section{Parallel Computing and Complexity Measures}
  \label{section:parmeasures}
  In Parallel Computing, the notion of time complexity of implementation
  has to be revisited. Now the time complexity is dependent on the
  number of processors available. Two key measures of
  an algorithm are \emph{work} complexity
  and \emph{depth} complexity. Work is defined
  to be the time (counted in number of operations)
  the algorithms needs, if it were executed on a single processor.
  Depth is defined as the longest chain of sequential data dependency.
  It can also be interpreted as the time needed if we had
  an unlimited number of processors.
  
  There are limited ways in calculating work and depth. Both are defined
  recursively over the work and depths of their subexpressions. Generally,
  work is the sum of operations involved in all of the subexpressions
  - while depths it  the maximum of number of operations involved in
  any particular chain of dependencies.
  We introduce the functions $\W(\cdot)$ and $\D(\cdot)$ which return work
  and depth, respectively. Sometimes we use syntactics constructs
  like $\W(f,x)$ to denote the work involved in \emph{applying}
  \c{f} on \c{x} - it does not include the work involved
  in calculating \c{x} in the first place. Sometimes,
  we will also simple denote $\W(n)$ or $\D(n,gmax)$ to emphasize,
  that the work and depth is dependent of the length of the input
  or of the parameter $gmax$. In these cases. the function refered to
  is clear from the context.
  Table \ref{table:workdepth} gives
  work and depth complexities for our parallel primitives.
  
  \begin{table}[h]
    \caption{Work and Depth complexities of general functions}
    \label{table:workdepth}
    \begin{center}
    \begin{tabular}{lll}
      \toprule
      function & work & depth \\
      \midrule
      (!:),indexP & 1 & 1 \\
      lengthP & 1 & 1 \\
      headP & 1 & 1 \\
      lastP & 1 & 1 \\
      g . f & $\W(g) + \W(f)$ & $\D(g) + \D(f)$ \\
      mapP f xs & 1 + $\W(xs) + \sum_{x \in xs}\W(f,x)$ & 1+ $\D(xs) + \max_{x \in xs}\D(f,x)$ \\
      zipWith f as bs & 1 + $\W(as) + \W(bs)$ & 1 + $\D(as) + \D(bs)$ \\
        & $+ \sum_{pairs (x,y) \in (xs,ys)}\W(f,x,y)$ & $ + \sum_{pairs (x,y) \in (xs,ys)}\D(f,x,y)$ \\
      sortP & $O(n \log n)$ & $O(\log n)$ \\
      sumP & n & $\log$ n \\
      concatP & 1 & 1 \\
      unconcatP & 1 & 1 \\
      replP &  n & 1 \\
    \end{tabular}
    \end{center}
  \end{table}
  
  Note that \c{mapP} has constant depth. 
  It embodies the idea, that the work or depth is the sum or maximum
  of the subexpressions neatly.
  
  Consider for example \c{ys = mapP inrc (replP n 1)}.
  Applying the formulas, we
  get complexities of
  \begin{equation*}
  \begin{split}
  \W(xs)
        & = 1 + \W(replP,n,1) + \sum_{x \in xs}\W(incr,x) \\
        & = 1 + n + n \\
        & \in O(n) \\
  \D(xs) & = 1 + \D(replP,n,1) + \max_{x \in xs}\D(incr,x) \\
      & = 1 + 1 + 1 \\
      & \in O(1) \\
  \end{split}
  \end{equation*}
  where $xs$ is the intermediate array created by \c{replP}.
  Note how we have constant depth!
  If we had as many PUs as our array is in size
  - we could assign each PU one element of the array and our operations
  would indeed require only constant time of local element allocation
  and function application.
  
  These measures, as introduced in \cite{Belloch1996}, work
  neatly within Nested Data Parallelism. Consider for example the
  expression \c{mapP (mapP incr) ass} where the
  nested array \c{ass} has dimensions $w \times h$.
  As we noted in section \ref{ndpintro}, within NDP,
  the nested parallel expression will be flattened into a
  flat array and mapped over only once. Therefore, we expect
  work in $O(w \cdot h)$ and depth in $O(1)$. And indeed - 
  this is what we get when applying our formulas.
  \begin{equation*}
  \begin{split}
  \W(mapP,mapP,incr)
        & = 1 + \sum_{as \in ass}\W(mapP,incr,as) \\
        & = 1 + \sum_{as \in ass}(1 + \sum_{a \in as} \W(incr,a)) \\
        & = 1 + \sum_{as \in ass}(1 + \sum_{a \in as} 1) \\
        & = 1 + \sum_{as \in ass}(1 + h) \\
        & = 1 + w \cdot (1 + h) \\
        & \in O(w \cdot h) \\
  \D(mapP,mapP,incr) & = 1 + \max_{as \in ass}\D(mapP,incr,as) \\
        & = 1 + \max_{as \in ass}(1 + \max_{a \in as} \D(incr,a)) \\
        & = 1 + \max_{as \in ass}(1 + \max_{a \in as} 1) \\
        & = 1 + \max_{as \in ass}2 \\
        & = 1 + 2 \\
        & \in O(1) \\
  \end{split}
  \end{equation*}
  
  Using these constructs, we have a useful tool in measuring
  programs complexity. In the complexities sections of
  the programs, mostly only the results - without their derivaiton -
  will be presented. More detailed derivations of the complexities shown
  in the algorithms sections can be found
  on the appendix.
   
\section{\algo}
  \label{section:hbalanceintro}
  This section will introduce \algo. It will state the problem and give
  an overview of its procedure.
  Suppose we have an $w \times h$-8-bit-gray-tone image with low contrast.
  \footnote{photo by Phillip Capper. Source: \url{https://en.wikipedia.org/wiki/File:Unequalized_Hawkes_Bay_NZ.jpg}}
      
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.45\textwidth]{img-org}
        \caption{An image with low constrast}
        \label{fig:img-org}
      \end{figure}
      Our goal is to make details more visible to the viewer. For that,
      Let's take a look at the histogram of the image.
      \footnote{made by used Wikipedia user \c{Jarekt}. Source: \url{https://en.wikipedia.org/wiki/File:Unequalized_Histogram.svg}}
      
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.45\textwidth]{hist-org}
        \caption{The histogram of the original image with absolute(red) and accumulative(black) count.}
        \label{fig:hist-org}
      \end{figure}
      
      A histogram shows the gray tone distribution of the image in interest.
      The x-axis denotes the gray tone and the y-axis denotes the
      number of pixels with a gray tone $x$ (red). The black curve denotes the
      total number of pixels with a gray tone $g \leq x$.

      We can now see, why details are difficult to recognize in our original image.
      The gray tones are tightly packed together and the
      entire image only uses values in the range [120..205].
      Histogram Balancing solves this problem by defining a mapping
      $f: Graytone \rightarrow Graytone$, such that the accumulating count
      of the resulting image increases as uniformly as possible from 0 to 255.
      The histogram \ref{fig:hist-eq}
      \footnote{made by used Wikipedia user \c{Jarekt}. Source: \url{https://en.wikipedia.org/wiki/File:Equalized_Histogram.svg}}
      shows our goal.
      
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.45\textwidth]{hist-eq}
        \caption{The histogram of the balanced image}
        \label{fig:hist-eq}
      \end{figure}
      
      Notice the new curve for the accumulating count(black). We can define
      this mapping by spreading out the gray tones such that more
      important gray tones get a larger range to occupy. We interpret
      a high histogram-value for a gray tone as a high importance.
      Given this interpretation, we can - build the accumulation histogram (1 and 2) - remembering which
      bar belongs to which gray tone, normalize(3) and scale(4) the bars to [0..255] - 
      and finally - assign each bar (and therefore its gray tone) a new gray tone
      using the bars location in the range [0..255](5).
      % TODO? I should include visual diagrams for this in the thesis.
      
      \paragraph{}
      To define the transformation $ hbalance: Image \rightarrow Image$ we need the following definitions:
      \begin{itemize}
        \item $Image := (Width,Height,Width \times Height \rightarrow Graytone)$:
          An image
        \item $Histogram_a: Graytone \rightarrow a$:
          A histogram  assigns a value of type $a$ to each gray tone. For $a = Int$,
          this becomes a (accumulated) histogram. For $a = Double$, we get
          a normalised histogram.
        \item $\gmax: Graytone$:
          It is the maximum gray tone for the data type in use. (e.g. 255 for 8-bit gray tones)
      \end{itemize}
      
      Furthermore, the method is broken down into the following functions:
      
      \begin{enumerate}
        \item $hist: Image \rightarrow Histogram_{Int}$
          It calculates the histogram of an image. (1)
        \item $accu: Histogram_{Int} \rightarrow Histogram_{Int}$
          It calculates the accumulating histogram from the original histogram. (2)
        \item $normalize: Int \times Int \times Histogram_{Int} \rightarrow Histogram_{Double}$
          It normalizes the bars (in the range defined by the first two arguments) to a range from 0 to 1. (3)
          The arguments are denoted $a0$ and $agmax$.
          Normalisation is defined by mapping the histogram values by $x \mapsto \frac{(x - a0)}{agmax - a0}$.
          The second argument (called $agmax$) denotes the number of total pixels in the image.
          \comment{
            For an accumulated histogram $a:Histogram_{Int}$ it can be calculated with
            $a(gmax)$ or $a(g')$ where $g'$ is highest gray tone in the image.
            Both are equal, so the implementations may take the liberty to use any of these formulas.
          }
        \item $scale: Graytone \times Histogram_{Double} \rightarrow Histogram_{Int}$
          It scales the normalized values to the maximum gray tone (gmax) and rounds down to the nearest integer.
          Scaling is defined by mapping the histogram values by $x \mapsto \left \lfloor{x \cdot gmax}\right \rfloor $.
        \item $apply: Histogram_{Int} \times Image \rightarrow Image$
          maps each gray tone to its new value as dictated by the first argument. (5)
      \end{enumerate}
      
      Given these functions we can define $hbalance: Image \rightarrow Image$ as:
      \begin{equation*}
      \begin{split}
          h & := hist(img) \\
          a & := accu(h) \\
          s & := scale(gmax,n) \\
          n & := normalize(a(0), a(gmax), a) \\
        hbalance(img) & := apply(s,img) \\
      \end{split}
      \end{equation*}
      
      Concrete implementations will be given in the coming chapters.
      Applying the algorithm to our initial image gives \ref{fig:img-eq}
      \footnote{photo by Phillip Capper. Source: \url{https://en.wikipedia.org/wiki/File:Equalized_Hawkes_Bay_NZ.jpg}}
      . Details are more distinguishable in our new image.
      
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.45\textwidth]{img-eq}
        \caption{The equalized image}
        \label{fig:img-eq}
      \end{figure}
    
  \algo{} is a frequently used algorithm in image processing. It is 
  one of first methods applied on images to decrease their complexity
  in sophisticated image processing pipelines.
  
  \paragraph{}
  After being exposed to the bread and butter, we are now ready to tackle the
  implementations \seq, \man, \ndpn and \ndpv!
  
  % TODO: cite a book for its uses and the algorithm itself
    
  \comment{
    Prefix sum is a very common operation in computer science. It is a special
      case of scanning through a ordered container from left ro right applying a binary
      associative function \c{f}.
      It is defined as:
        $$ scanl(f,z,[a_1,a_2,...,a_n])
           := [f(z,a_1),f(f(z,a_1),a_2),...,f(f(...f(f(z,a_1),a_2)...,a_{n-1}),a_n)]
        $$
      An example shall be $scanl(+,0,[1,2,2,3,-2]) = [1,3,5,8,6]$. In some definitions
      the first element is $z$. This is however not the definition we need here.
  }
