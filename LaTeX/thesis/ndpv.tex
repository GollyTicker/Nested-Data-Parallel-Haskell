\epigraph{\emph{
"Now I will have less distraction."
}}{
Leonhard Euler, after loosing \\ his right eyesight
}


This chapter will apply the core transformation and optimisations
offered by Nested Data Parallelism in Haskell.
This chapter will go through the transformations first and then present the
final program \ndpv. The program would have been the results of the compilers \textit{automatic}
optimisations before it would be translated into machine code and finally executed.
At the end of this chapter, complexity measures for \ndpv are given.


\section{Transformations}
  As presented in section \ref{section:ndpintro}, the compiler applies
  three phases of program transformations:
  \begin{enumerate}
    \item \emph{Vectorization} - Flattening of array representations
      and flattening of nested parallel functions.
    \item \emph{Communication Fusioning} - Inlining of parallel
      functions and the use of Rewrite Rules
      to eliminate communication.
    \item \emph{Stream Fusioning} - Inlining of local traversals and
      the use of Rewrite Rules to reduce the number of traversals.
  \end{enumerate}
  
  The compiler begins with Vectorization.
  
  \subsection{Vectorization}
    Applying the vectorization procedure as described in \cite{Harness2008} yields the following code:
    % After Vectorization:
    \begin{lstlisting}
hbalance img :: PA (PA Int)
hbalance img = 
let a = scanlPS plusInt 0
            . sparseToDensePS (plusInt gmax 1) 0
            . (\g -> ATup2 (headPL g) (lengthPL g))
            . groupPS
            . sortPS
            . concatPS
            $ img
    n = lengthPS a
    gs = floorDoubleL
           . multDoubleL (int2DoubleL (replPS n gmax))
           . divL
               (minusL
                (int2DoubleL a)
                (replPS n (int2Double (headPS a)))
               )
           . replPS n
           $ minusDouble (int2Double (lastPS a)) (int2Double (headPS a))
in unconcatPS img
     . indexPL (expandPS img gs)
     . concatPS
     $ img
    \end{lstlisting} % (expandPS img gs)
    Various functions (like \c{hist} and \c{accu}) have been inlined and are tightly packed together here.
    The program also replaced the nested \pan by flat \pav.
    It also replaced polymorphic functions like \c{fromIntegral}
    by specific monomorphic primitive machine functions like \c{int2Double}.
    
    Starting, lines 9 to 4 describe the calculation of the histogram.
    It's only difference is the use of vectorized scalar functions (e.g.\c{groupPS}). These functions operate of the efficient flat
    representation instead of the nested representation.
    
    After that in line 4, the accumulated histogram is calculated.
    
    Lines 19 to 13 and 21 to 11 describe the normalisation and scaling of the gray tones respectively.
    The vectorized code uses lifted arithmetic functions (like \c{floorDoubleL}) that
    operate over arrays.
    The normalisation constants \c{gmax'},\c{a0} and \c{divisor}
    have been inlined and are replicated\footnote{\c{replPS n x} creates an array of length n - all containing the element x. It has the type \type{Int -> a -> PA a}}
    to the length of the gray tone array before the lifted arithmetic operations are applied.
    
    
    Finally, lines 32 to 20 describe the mapping of the images gray tones.
    The nested parallel operation \c{mapP (mapP (!a))} - formerly a part of \c{apply} - 
    has now been flatten to use a lifted parallel operation, namely \c{indexPL}, over a \textbf{flat} array of
    the pixels of the image. This is an embodiment of the key insight in Nested Data Parallelism.
    
    The compiler however had to add an expression \c{expandPS img gs} for it to be
    correct. That is an artefact of the actual flattening procedure.
    It is responsible for the redistribution of the \c{gs} array. It does not affect the
    overall situation (in terms of work and depth) and therefore
    it is not further discussed.
    
    In total, the program has a few smaller Constant Factors now. This is mainly due to the elimination of nested data structures
    and operations. However, there is still much room for improvement.
    
  \subsection{Communication Fusioning}
    
    Communication Fusioning consists of inlining definitions of parallel functions and using rewrite rules to eliminate
    unnecessary communication. Applying this transformation changes the definition of the histogram \c{a} and the
    gray tones \c{gs}. They are separately discussed below.
    
    \subsubsection{Histogram calculation}
      The new definition of the accumulated histogram calculation is given below:
      \begin{lstlisting}
let a = joinD                           -- scanlPS ends
          . mapD (\(as,a) -> mapS (plusInt a) as)
          . propagateD plusInt 0
          . mapD (scanlS plusInt 0)       -- scanlPS begins; fused
          . sparseToDenseD (plusInt gmax 1) 0 -- sparseToDensePS ends
          . splitSparseD (plusInt gmax 1)     
          . joinD                             -- sparseTodensePS begins
          . mapD tripletToATup2               -- fused lambda and groupPS 
          . segdSplitMerge 0                  -- workhorse of groupPS
          . sortPS
          . concatPS
          $ img
      \end{lstlisting}
      One can firstly observe the occurrence of distributed functions (with a \c{-D} suffix). They operate either
      on each PU locally (as with \c{mapD}) or implement some specific inter-PU calculation (as does \c{propagateD}).
      These functions are the result of inlining various parallel functions and eliminating communication.
      The correspondence to their original functions is given as comments in the code. Only sortPS and concatPS
      are unchanged. A real compiler would have inlined their definitions and looked for optimisations.      
      
      Aside from them, inlining \c{scanlPS} and \c{sparseToDensePS}
      \footnote{It's definition is given in the appendix \ref{lst:sparsetodenseps}.} exposes their internals.
      In-between both of the functions, there was a composition of the distribution primitives - namely \c{splitD . joinD}.
      Applying the rewrite rule "splitD/joinD" eliminated a point of synchronisation.
      
      This leaves \c{propagateD} as the only inter-PU communication in-between the splitting of the
      sparse array and joining the histogram at the end (line 1).
      
      The expression \c{mapP (\lam g -> (headP g,lengthP g)) . groupPS}
      was involved in a rather special fusion. Essentially, the lambda expression was
      was applied to the result of groupPS. This enabled further communication
      fusion and created the local operation \c{tripletToATup2}
      \footnote{\c{tripletToATup2 :: LinkedList (Int,Int,Int) -> PA (Int,Int)}.}
      . It creates the
      local chunks of the sparse-array directly. \c{segdSplitMerge} does the
      actual work of the distributed grouping\footnote{as explained in chapter \ref{chapter:ndpn}}.
      
    \subsubsection{Normalisation and Scaling}
    The new code for normalisation and scaling is given below:
    \begin{lstlisting}
let n = lengthPS a
      gs = joinD . mapD f . splitD $ a
      f = (\gmax' divisor a0 x ->
            floorDoubleS
              (multDoubleS
                (divDoubleS
                  (minusDoubleS
                    (int2DoubleS x)
                    a0)
                  divisor)
                gmax')
            )
            $ ( replD n . int2Double $ gmax )
            $ ( replD n
                . minusDouble (int2Double (lastPS a))
                . int2Double . headPS $ a )
            $ ( replD n . int2Double . headPS $ a )
    \end{lstlisting}
    The definitions of the lifted functions are similar to these two examples:
    \begin{lstlisting}
floorDoubleL = joinD . mapD floorDoubleS . splitD
multDoubleL as = joinD . zipWithD multDoubleS (splitD as) . splitD
    \end{lstlisting}
    Inlining these functions created \emph{five} pairs of \c{splitD . joinD} - which were then immediately
    eliminated using the "splitD/joinD" rule.
    
    After that, a cascade of rewrite rules fired and propagated the normalisation and scaling
    constants into local computation. A sophisticated combination of the rules
    "mapD/zipWithD", "splitD/replPS", "mapD/replD" and "ZipReplSplit" resulted in
    the code given above.
    
    Operationally, there is an important change in the normalisation and scaling.
    Although, constants (like \c{int2Double \$ gmax}) are still being replicated into arrays
    before applying the arithmetic mapping - now the replication
    is only limited to the local PU. This is different than before, when
    the constants were replicated \emph{globally} and subsequently split.
    
    In terms of speed, communication fusion eliminated \emph{six} points of synchronisation.
    It greatly reduced constant factors in its complexity by pushing
    replications from global redistributions into local PU operations.
    Stream Fusion further improves this.
    
    \nomenclature{Constant Factors}{
    High Constant factors increase the running time of a
    program that are hidden inside Landau-Notation $O(\cdot)$.
    They are important for decreasing practical running time
    }
    
  \subsection{Stream Fusioning}
    Stream Fusioning is the final step of optimisation. Applying it improves
    the normalisation and scaling only. The new code is given below:
     \footnote{\c{flip} were not actually inserted by the compiler.
     However, the nesting of functions is easier to display
     using \c{flip}s.}
    \begin{lstlisting}
let a0      = int2Double . headPS $ a 
      divisor = minusDouble (int2Double (lastPS a))
                    . int2Double . headPS $ a
      gmax'   = int2Double $ gmax
      normScale = floorDouble
                      . (flip multDouble) gmax'
                      . (flip divDouble) divisor
                      . (flip minusDouble) a0
                      . int2Double
      gs = joinD . mapD (mapS normScale) . splitD $ a
     \end{lstlisting}
     The replications have been removed entirely.
     Constants are calculated first and then used in \c{normScale}
     to apply the arithmetic calculation.
     The arithmetic functions also have been merged together into
     single function \c{normScale}. This function is now applied element-wise
     on each value in each of the local chunks of the entire histogram array.
     
     The code is the result of inlining the local sequential functions (like \c{multDoubleS})
     and subsequent stream fusion.
     For example, \c{multDoubleS} and \c{floorDoubleS} are defined as:
     \begin{lstlisting}
floorDoubleS :: Vector Double -> Vector Int
floorDoubleS = unstream . mapSt floorDouble . stream

multDoubleS :: Vector Double -> Vector Double -> Vector Double
multDoubleS as = unstream . zipWithSt multDouble (stream as) . stream
     \end{lstlisting}
     Inlining these definitions creates expressions of \c{unstream . stream}. Applying
     the "unstream/stream" rule and a few other rules analogous to
     Communication Fusioning finally propagates the constants outside
     of any traversals.
     
     The transformation is over now. The next section will give an overview of the results.
         
\section{Final Program}
  Summing up the steps, the compiler gives the following optimised code for \ndpv:
  \begin{lstlisting}
type Image = PA (PA Int)
type Hist  = PA Int

hbalance :: Image -> Image
hbalance img =
let a :: Hist
      a = joinD
            . mapD (\(as,a) -> mapS (plusInt a) as)
            . propagateD plusInt 0
            . mapD (scanlS plusInt 0)
            . sparseToDenseD (plusInt gmax 1) 0
            . splitSparseD (plusInt gmax 1)
            . joinD
            . mapD tripletToATup2
            . segdSplitMerge 0
            . sortPS
            . concatPS
            $ img
      n :: Int
      n = lengthPS a
      
      a0, divisor, gmax' :: Double
      a0      = int2Double . headPS $ a
      divisor = minusDouble (int2Double (lastPS a))
                    . int2Double . headPS $ a
      gmax'   = int2Double gmax
      
      normScale :: Int -> Int
      normScale = floorDouble
                        . (flip multDouble) gmax'
                        . (flip divDouble) divisor
                        . (flip minusDouble) a
                        . int2Double
        
      gs :: Hist
      gs = joinD . mapD (mapS normScale) . splitD $ a
      
 in  unconcatPS img
       . indexPL (expandPS img gs)
       . concatPS
       $ img
  \end{lstlisting} %  (expandPS img gs)
  On the surface, the algorithm works quite similar to a direct implementation of \ndpn.
  First the histogram is calculated (lines 18 to 11) and accumulated (lines 10 - 7).
  Then the constants \c{a0},\c{divisor} and \c{gmax'} are calculated globally and distributed
  to each PU (lines 22 to 33). After that, each PU applies the normalisation and scaling transformations (line 36).
  The mapping array \c{gs} is then finally used to map each gray tone to its new value (lines 41 to 38).
  The gray tone mapping is an example of the flattening of nested parallel functions in NDP.
  
  All in all, \ndpv offers a few advantages over \ndpn:
  \begin{itemize}
    \item a decreased number of communication and synchronisation points
    \item flat data structures and flat operations further decrease constant factors
    \item Inlining and optimisation fused
          normalisation and scaling together - even though
          they were separated in \ndpn. The programmer did not
          need to fuse them manually. This stands in contrast to the situation in \man.
    \item After writing \ndpn, there is no more work involved for the programmer in generating this optimised code.
  \end{itemize}
  Having transformed \ndpn to \ndpv,
  one is now ready to give a complexity analysis thereof.
  
\section{Complexities}
  The complexity analysis for work and depths remains similar to that of \ndpn.
  After all, both are the same algorithm. \ndpv is only better at it's constant factors.
  Let $n$ be the number of pixels in the image,
  then its complexity is given by:
  \footnote{
  The calculation of \c{a} is re-split into \c{hist} and \c{accu}.
  }
  \footnote{
  The reader might ask, why \c{propagateD} has higher depth than work 
  - and why work is simply 1. That is, because the value
  increases logarithmically with the number of PUs.
  Since in 'work' only a single processor is used,
  there is no propagation of values and the time is constant.
  }
  \begin{equation*}
  \begin{split}
  \W(w \times h,gmax)
        & = \W(hist) + \W(accu) + \W(gs) + \comment{\W(expandPS) +} \W(img') \\
        & \in O( (n \log n + gmax) + gmax + gmax \comment{ + 1} + n) \\
        & = O(n \log n + gmax) \\
  \D(w \times h,gmax)
      & = \D(hist) + \D(accu) + \D(gs) \comment{+ \D(expandPS)} + \D(img') \\
      & \in O(\log n + \log gmax + 1 + 1)  \\
      & = O(\log n + \log gmax) \\
  \end{split}
  \end{equation*}
  The use of the plentiful new functions does not change the overall situation compared to \ndpn.
  \ndpv has the same work and depth complexities as before.
  Being at $O(n \log n + gmax)$ in work
  the histogram calculation remains the most expensive when executed on a single processor.
  With increasing number of processors, the various logarithmic depth
  operations in the (accumulated) histogram calculation become the bottleneck.
  They require time logarithmic in the number of gray tones
  \c{gmax} and the number of pixels \c{n}.
  A overview of all the functions an their complexities can be found in the table \ref{complexities:ndpv}.
  
  \begin{table}[h]
    \caption{Complexities for \ndpv}
    \label{complexities:ndpv}
    \centering
    \begin{tabular}{lll}
        \toprule
        function or variable & $\W \in O(...)$           & $\D \in O(...)$ \\
        \midrule
        hbalance        & $n \log n + gmax$ & $\log n + \log gmax$ \\
        \midrule
        hist            & $n \log n + gmax$& \log n \\
        concatPS        & 1                   & 1 \\
        sortPS          & $n \log n$          & $\log n$ \\
        segdSplitMerge  & $n \log n$          & $\log n$ \\
        mapD tripletToATup2  & gmax           & 1 \\
        joinD xs        & gmax                & 1 \\
        \midrule
        accu            & gmax                & $\log gmax$ \\
        sparseToDenseD  & gmax                & 1 \\
        splitSparseD    & gmax                & 1 \\
        mapD scanlS     & gmax                & 1 \\
        propagateD      & 1                   & $\log gmax$ \\
        mapD (mapS plusInt) & gmax            & 1 \\
        joinD           & gmax                & 1 \\
        \midrule
        as              & gmax                & 1 \\
        splitD          & gmax                & 1 \\
        mapD normScale  & gmax                & 1 \\
        joinD           & gmax                & 1 \\
        \midrule
        expandPS        & n & 1 \\
        \midrule
        img'            & n                   & 1 \\
        concatPS        & 1                   & 1 \\
        indexPL         & n                   & 1 \\
        unconcatPS      & 1                   & 1 \\
    \end{tabular}
  \end{table}
  
  \paragraph{}
    The implementation and analysis of the programs \seq, \man, \ndpn and \ndpv
    have been given.
    The next chapter with the discussion of the results.
    

